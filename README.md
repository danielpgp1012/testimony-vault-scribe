## Testimony Vault Scribe

A minimal full‑stack app to upload audio testimonies, auto‑transcribe (Whisper), summarize, embed, and search (keyword + semantic) with Supabase + pgvector.

- **Frontend**: React + Vite — http://localhost:5173
- **API**: FastAPI — http://localhost:8000 (docs: http://localhost:8000/docs)
- **Worker**: Celery + Redis (async transcription/summarization)
- **DB**: Supabase Postgres + pgvector

### Quick start

1) Prereqs: Node 18+, Docker, OpenAI API key, Supabase project (pgvector enabled).

2) Create `.env` (root):
```env
REDIS_URL=redis://redis:6379/0
OPENAI_API_KEY=your_openai_api_key
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_anon_key
AUDIO_UPLOAD_DIR=/shared/tmp
# optional
SKIP_CLIENT_INIT=false
CELERY_LOG_LEVEL=info
```

3) Start:
```bash
docker compose up -d   # api, worker, redis
npm install && npm run dev
```

### How it works

1. Upload MP3 → API stores row with `transcript_status="pending"` and saves audio to `/shared/tmp`.
2. Celery task transcribes with `whisper-1` (primary language: es).
3. Worker summarizes the transcript using a fixed system prompt (v2) and stores `summary` and `summary_prompt_id`.
4. Worker embeds the full summary (summary + hashtags collapsed to one line) with `text-embedding-3-small` and upserts one vector per testimony.
5. Search options:
   - Keyword: `GET /testimonies/search/{query}`
   - Semantic: `GET /testimonies/semantic-search?q=...&k=10` (query embedded on-the-fly; matched in Postgres via RPC)

### Summarization configuration

Environment overrides (defaults in code):
- `SUMMARY_MODEL` (default `gpt-4.1-mini`)
- `SUMMARY_TEMPERATURE` (default `0.3`)
- `SUMMARY_MAX_TOKENS` (default `250`)
- `SUMMARY_PROMPT_VERSION` (default `v2`)

### Embeddings and semantic search

- Embedding model: `text-embedding-3-small` (1536 dims). One vector per testimony from the summary text.
- Enable pgvector and create the table and RPC in Supabase (SQL editor):
```sql
create extension if not exists vector with schema extensions;

create table if not exists public.testimony_embeddings (
  id bigint generated by default as identity primary key,
  testimony_id bigint not null references public.testimonies(id) on delete cascade,
  embedding vector(1536) not null,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

create unique index if not exists testimony_embeddings_unique on public.testimony_embeddings (testimony_id);
create index if not exists testimony_embeddings_ivfflat on public.testimony_embeddings using ivfflat (embedding vector_cosine_ops) with (lists = 100);

create or replace function public.match_testimonies(
  query_embedding vector(1536),
  match_count int
)
returns table (
  id bigint,
  testimony_id bigint,
  similarity double precision,
  summary text,
  transcript text,
  recorded_at timestamptz,
  church_id text,
  tags text[]
) as $$
  select
    te.id,
    te.testimony_id,
    1 - (te.embedding <=> query_embedding) as similarity,
    t.summary,
    t.transcript,
    t.recorded_at,
    t.church_id,
    t.tags
  from public.testimony_embeddings te
  join public.testimonies t on t.id = te.testimony_id
  order by te.embedding <=> query_embedding
  limit match_count;
$$ language sql stable;
```

Backfill for existing rows (optional): see scripts in `backend/migrations/`.

### API overview

- POST `/testimonies` (multipart: `file`, `church_id?`, `recorded_at?`, `tags?`)
- GET `/testimonies`
- GET `/testimonies/search/{query}`
- GET `/testimonies/semantic-search?q=...&k=10`
- GET `/tasks/{task_id}`
- GET `/worker/stats`

### License

MIT
